{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55bc9f5b-f0b1-4945-b7a1-cf7082ad5f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kolli\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3dfa66e5-937b-47a5-97f1-68d99e1143ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read excel\n",
    "excel_file = \"Input.xlsx\"\n",
    "df = pd.read_excel(excel_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0496c7cd-e703-428e-8efa-9852f5c5759b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41523868-83c4-4f2b-ad5b-7d959614ddcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f0468b0-80d6-4685-9260-e6966947bbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to scrape webpage and save content into a text file\n",
    "def content_files(url_id, url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Extract title\n",
    "    title_tag = soup.find('title')\n",
    "    if title_tag:\n",
    "        title = title_tag.get_text()\n",
    "    else:\n",
    "        title = \"Title not found\"\n",
    "    \n",
    "    # Extract content\n",
    "    content = \"\"\n",
    "    for paragraph in soup.find_all('p'):\n",
    "        content += paragraph.get_text() + \"\\n\"\n",
    "    \n",
    "    with open(f'extracted_files/{url_id}.txt', 'w', encoding='utf-8') as f:\n",
    "        f.write(title + '\\n\\n')\n",
    "        f.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36d0b13d-3651-4dc2-b738-5008f7126196",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('extracted_files'):\n",
    "    os.makedirs('extracted_files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ed1890-ef91-464d-9d0c-7b8ebad65079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping and saving content for each URL\n",
    "for index, row in df.iterrows():\n",
    "    content_files(row['URL_ID'], row['URL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b459b3e-fa63-4687-9f21-6a60da80d9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_d = \"StopWords\"\n",
    "ext_dir = \"extracted_files\"\n",
    "\n",
    "stop_words = set()\n",
    "for files in os.listdir(stopwords_d):\n",
    "  with open(os.path.join(stopwords_d,files),'r',encoding='ISO-8859-1') as f:\n",
    "    stop_words.update(set(f.read().splitlines()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d66c8f-107d-4a45-8fe3-4b75723f984a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aef87b49-2e64-4b64-8abf-17d73a79d0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "docs = []\n",
    "for file in os.listdir(ext_dir):\n",
    "  with open(os.path.join(ext_dir, file), 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    text = re.sub(r'[^\\w\\s.]','',text)\n",
    "#tokenize the given text file\n",
    "    words = word_tokenize(text)\n",
    "# remove the stop words from the tokens\n",
    "    filtered_text = [word for word in words if word.lower() not in stop_words]\n",
    "# add each filtered tokens of each file into a list\n",
    "    docs.append(filtered_text)\n",
    "\n",
    "    sentences = text.split('.')\n",
    "    num_sentences = len(sentences)\n",
    "    words = [word  for word in text.split() if word.lower() not in stop_words ]\n",
    "    num_words = len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9736f685-d847-4ef8-aad9-4e4f57e75006",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6f1d575-5d21-479d-9d53-85087e0117ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_words = set()\n",
    "neg_words = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b833b2ea-1f28-4649-a42f-9b0d59f07c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_dir = \"MasterDictionary\"\n",
    "\n",
    "for file in os.listdir(master_dir):\n",
    "  if file =='positive-words.txt':\n",
    "    with open(os.path.join(master_dir,file),'r',encoding='ISO-8859-1') as f:\n",
    "      pos_words.update(f.read().splitlines())\n",
    "  else:\n",
    "    with open(os.path.join(master_dir,file),'r',encoding='ISO-8859-1') as f:\n",
    "      neg_words.update(f.read().splitlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8e8484-ef1d-4105-8a07-f1cf2ba87fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a82beb-7830-4f31-9360-418b18eb1c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02b61329-ae01-4d9d-a117-c61ad93ce94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_words = []\n",
    "negative_words =[]\n",
    "pos_score = []\n",
    "neg_score = []\n",
    "polarity_score = []\n",
    "subj_score = []\n",
    "avg_sentence_length = []\n",
    "perc_of_complex_words  =  []\n",
    "fog_index = []\n",
    "complex_word_count =  []\n",
    "avg_syll_word_count =[]\n",
    "word_count = []\n",
    "average_word_length = []\n",
    "pp_count = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a28c562-558b-4c3c-93e1-14cf26534ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(docs)):\n",
    "  positive_words.append([word for word in docs[i] if word.lower() in pos_words])\n",
    "  negative_words.append([word for word in docs[i] if word.lower() in neg_words])\n",
    "  pos_score.append(len(positive_words[i]))\n",
    "  neg_score.append(len(negative_words[i]))\n",
    "  polarity_score.append((pos_score[i] - neg_score[i]) / ((pos_score[i] + neg_score[i]) + 0.000001))\n",
    "  subj_score.append((pos_score[i] + neg_score[i]) / ((len(docs[i])) + 0.000001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26bfabd4-1e57-4c57-abcd-389d6ad7ce68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_metrics(file):\n",
    "    for file in os.listdir(ext_dir):\n",
    "      with open(os.path.join(ext_dir, file), 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "        sentences = text.split('.')\n",
    "        num_sentences = len(sentences)\n",
    "        words = [word  for word in text.split() if word.lower() not in stop_words]\n",
    "        num_words = len(words)\n",
    "\n",
    "        #complex words\n",
    "        complex_words = []\n",
    "        for word in words:\n",
    "          vowels = 'aeiou'\n",
    "          syllable_count_word = sum(1 for letter in word if letter.lower() in vowels)\n",
    "          if syllable_count_word > 2:\n",
    "            complex_words.append(word)\n",
    "\n",
    "\n",
    "        #syllable words\n",
    "        syllable_count = 0\n",
    "        syllable_words =[]\n",
    "        for word in words:\n",
    "          if word.endswith('es'):\n",
    "            word = word[:-2]\n",
    "          elif word.endswith('ed'):\n",
    "            word = word[:-2]\n",
    "          vowels = 'aeiou'\n",
    "          syllable_count_word = sum( 1 for letter in word if letter.lower() in vowels)\n",
    "          if syllable_count_word >= 1:\n",
    "            syllable_words.append(word)\n",
    "            syllable_count += syllable_count_word\n",
    "\n",
    "\n",
    "        avg_sentence_len = num_words / num_sentences\n",
    "        avg_syll_word_count = syllable_count / len(syllable_words)\n",
    "        perc_complex_words  =  len(complex_words) / num_words\n",
    "        fog_index = 0.4 * (avg_sentence_len + perc_complex_words)\n",
    "\n",
    "        return avg_sentence_len, len(complex_words), avg_syll_word_count, perc_complex_words, fog_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "985505fc-06b4-4dfa-97ad-48fc3b5091be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def len_words(file):\n",
    "  with open(os.path.join(ext_dir, file), 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    text = re.sub(r'[^\\w\\s]', '' , text)\n",
    "    words = [word  for word in text.split() if word.lower() not in stop_words]\n",
    "    length = sum(len(word) for word in words)\n",
    "    average_word_length = length / len(words)\n",
    "  return len(words),average_word_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f239423f-a846-462c-b165-bc5323520e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def personal_pronouns(file):\n",
    "  with open(os.path.join(ext_dir,file), 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    personal_pronouns = [\"I\", \"we\", \"my\", \"ours\", \"us\"]\n",
    "    count = 0\n",
    "    for pronoun in personal_pronouns:\n",
    "      count += len(re.findall(r\"\\b\" + pronoun + r\"\\b\", text)) # \\b is used to match word boundaries\n",
    "  return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "568eaf21-f1b9-49e1-a7cd-49c43481e822",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in os.listdir(ext_dir):\n",
    "    a,b,c,d,e = file_metrics(i)\n",
    "    avg_sentence_length.append(a)\n",
    "    complex_word_count.append(b)\n",
    "    avg_syll_word_count.append(c)\n",
    "    perc_of_complex_words.append(d)\n",
    "    fog_index.append(e)\n",
    "\n",
    "    a, b = len_words(i)\n",
    "    word_count.append(a)\n",
    "    average_word_length.append(b)\n",
    "\n",
    "    x = personal_pronouns(i)\n",
    "    pp_count.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "632c5fbc-1e23-48c7-b0f8-31b1c249f88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = {'POSITIVE SCORE':pos_score,\n",
    "            'NEGATIVE SCORE':neg_score,\n",
    "            'POLARITY SCORE':polarity_score,\n",
    "            'SUBJECTIVITY SCORE':subj_score,\n",
    "            'AVG SENTENCE LENGTH':avg_sentence_length,\n",
    "            'PERCENTAGE OF COMPLEX WORDS':perc_of_complex_words,\n",
    "            'FOG INDEX':fog_index,\n",
    "            'AVG NUMBER OF WORDS PER SENTENCE':avg_sentence_length,\n",
    "            'COMPLEX WORD COUNT':complex_word_count,\n",
    "            'WORD COUNT':word_count,\n",
    "            'SYLLABLE PER WORD':avg_syll_word_count,\n",
    "            'PERSONAL PRONOUNS':pp_count,\n",
    "            'PERSONAL PRONOUNS':average_word_length}\n",
    "\n",
    "existing_df = pd.read_excel('Output Data Structure.xlsx')\n",
    "first_two_columns = existing_df.iloc[:, :2]\n",
    "new_df = pd.DataFrame(variables)\n",
    "final_df = pd.concat([first_two_columns, new_df], axis=1)\n",
    "final_df.to_csv('output.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2514db8d-8ae8-43c7-9f7a-dd0ad84341f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
